{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "In case you are using a single-node cluster, executing this cell is essential, as otherwise, SparkContext put the sc.master on 'yarn' which you don't have. The result would be you'll never see a collect() to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc._conf.getAll()\n",
    "sc.stop()\n",
    "conf = SparkConf().setMaster('local')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visits = sc.parallelize([('index.html', '1.2.3.4'),\n",
    "                         ('about.html', '3.4.5.6'),\n",
    "                         ('index.html', '1.3.3.1')])\n",
    "\n",
    "pageNames = sc.parallelize([('index.html', 'Home'), \n",
    "                            ('about.html', 'About')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vj = visits.join(pageNames)\n",
    "vj.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vc = visits.cogroup(pageNames)\n",
    "vc.collectAsMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### second example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()\n",
    "sc = SparkContext(master='local', appName='WordCount', sparkHome='/usr/lib/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lines = sc.textFile('gs://datathinks-home/hamlet.txt')\n",
    "lines = sc.textFile('hdfs:///user/root/test.txt')\n",
    "lines = lines.flatMap(lambda line: line.split(' ')) \\\n",
    "             .map(lambda word: (word, 1))\\\n",
    "             .reduceByKey(lambda x, y: x + y)\n",
    "# lines.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil rm -r gs://saberbf0098/sparkResults/wordCount/\n",
    "lines.saveAsTextFile('gs://saberbf0098/sparkResults/wordCount/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil cat gs://saberbf0098/sparkResults/wordCount/part-00000 | head -5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3 Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put /home/saberbf/transition-matrix.txt  /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['S0', 'S1', 'S2', 'S3', 'S4', 'Sf']\n",
    "links = sc.parallelize(map(lambda s: (s, states), states)) \\\n",
    "# links.collect()\n",
    "\n",
    "ranks = sc.parallelize(states).map(lambda s: (s, 1.0))\n",
    "# ranks.collect()\n",
    "\n",
    "wordProbs = sc.parallelize([('S0',{}), \\\n",
    "                            ('S1', {'Tom':1/5, 'John':1/5, 'Mary':1/5, 'Alice':1/5, 'Jerry':1/5}), \\\n",
    "                            ('S2',{'a':3/8, 'the':4/8, 'that':1/8}),\\\n",
    "                            ('S3',{'bit':1/6, 'saw':1/6, 'ate':1/6, 'played':1/6, 'hit':1/6, 'gave':1/6}), \\\n",
    "                            ('S4', {'cat':1/6, 'dog':1/6, 'car':1/6, 'bed':1/6, 'apple':1/6, 'pen':1/6}), \\\n",
    "                            ('Sf', {})])\n",
    "# wordProbs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('hdfs:///user/root/transition-matrix.txt')\n",
    "header = lines.first().split('\\t')\n",
    "def czip(x):\n",
    "    return tuple(zip(states, x[1:]))\n",
    "    \n",
    "trans = lines.map(lambda line: line.split('\\t')) \\\n",
    "             .filter(lambda line: line != header)\\\n",
    "             .map(czip).flatMap(lambda x:x) \\\n",
    "             \n",
    "\n",
    "numlinks = trans.filter(lambda x: x[1] !='0').map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x+y)\n",
    "trans = trans.mapValues(float).reduceByKey(lambda x, y: x+y) \n",
    "\n",
    "numlinks.collect()\n",
    "\n",
    "ranks.join(numlinks.join(trans)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "pre_ranks = ranks\n",
    "diff = 1\n",
    "def compute_contribs(pair):\n",
    "    [url, [rank, [numlinks,tran]]] = pair  # split key-value pair\n",
    "    return [(url, rank * tran / numlinks)]\n",
    "\n",
    "def rdd_diff(pair):\n",
    "    (url, (pre, cur)) = pair\n",
    "    return [abs(cur-pre)]\n",
    "while diff > 0.01:\n",
    "    contribs = ranks.join(numlinks.join(trans)).flatMap(compute_contribs)\n",
    "    ranks = contribs.reduceByKey(lambda x, y: x + y) \\\n",
    "                    .mapValues(lambda x: 0.15 + 0.85 * x)\n",
    "    diff = pre_ranks.join(ranks).flatMap(rdd_diff).mean()\n",
    "    pre_ranks = ranks\n",
    "    iters +=1\n",
    "\n",
    "ranks = ranks.sortBy(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil rm -r gs://saberbf0098/sparkResults/HW2/\n",
    "ranks.saveAsTextFile('gs://saberbf0098/sparkResults/HW2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nNumber of iterations: ',iters)\n",
    "print('Ranks: ')\n",
    "ranks.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_word_prob(pair):\n",
    "    [jar, [wordProbs, rank]] = pair\n",
    "    return [(jar, [(word,prob * rank)]) for (word,prob) in wordProbs.items()]\n",
    "\n",
    "print('List of Word Probabilities in each Bucket:') \n",
    "wordProbs.join(ranks).flatMap(calc_word_prob).reduceByKey(lambda x,y: x+y)\\\n",
    "                     .sortBy(lambda x: x[0]).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3 Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "|Invoice|StockCode|         Description|Quantity|   InvoiceDate|Price|Customer ID|       Country|\n",
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "| 489434|    85048|15CM CHRISTMAS GL...|      12|12/1/2009 7:45| 6.95|      13085|United Kingdom|\n",
      "| 489434|   79323P|  PINK CHERRY LIGHTS|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|\n",
      "| 489434|   79323W| WHITE CHERRY LIGHTS|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|\n",
      "| 489434|    22041|\"RECORD FRAME 7\"\"...|      48|12/1/2009 7:45|  2.1|      13085|United Kingdom|\n",
      "| 489434|    21232|STRAWBERRY CERAMI...|      24|12/1/2009 7:45| 1.25|      13085|United Kingdom|\n",
      "| 489434|    22064|PINK DOUGHNUT TRI...|      24|12/1/2009 7:45| 1.65|      13085|United Kingdom|\n",
      "| 489434|    21871| SAVE THE PLANET MUG|      24|12/1/2009 7:45| 1.25|      13085|United Kingdom|\n",
      "| 489434|    21523|FANCY FONT HOME S...|      10|12/1/2009 7:45| 5.95|      13085|United Kingdom|\n",
      "| 489435|    22350|           CAT BOWL |      12|12/1/2009 7:46| 2.55|      13085|United Kingdom|\n",
      "| 489435|    22349|DOG BOWL , CHASIN...|      12|12/1/2009 7:46| 3.75|      13085|United Kingdom|\n",
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option('header','true').load('gs://datathinks-home/online_retail_II.csv')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records befor duplicate removal:  1067371\n",
      "Number of records after duplicate removal:  1033034\n",
      "Number of duplicate records removed:  34337\n",
      "Number of Null records:  235151\n",
      "Number of records befor missing value imputation:  1033034\n",
      "Number of records after missing value imputation:  797883\n",
      "+-------+---------+--------+---------------+-----+----------+\n",
      "|Invoice|StockCode|Quantity|    InvoiceDate|Price|CustomerID|\n",
      "+-------+---------+--------+---------------+-----+----------+\n",
      "| 489520|    22080|      10|12/1/2009 11:41| 1.65|     14911|\n",
      "| 489522|    22300|       3|12/1/2009 11:45| 2.55|     15998|\n",
      "| 489532|    16235|      60|12/1/2009 11:58| 0.21|     13394|\n",
      "| 489537|   72801G|       3|12/1/2009 12:14| 1.25|     14040|\n",
      "| 489539|    20692|      96|12/1/2009 12:18| 3.75|     15061|\n",
      "+-------+---------+--------+---------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select\\rename columns\n",
    "df = df.withColumn(\"CustomerID\", df[\"Customer ID\"]).drop(\"Customer ID\", \"Country\", \"Description\")\n",
    "n_records_before_cleaning = df.count()\n",
    "# remove duplicate records\n",
    "df = df.distinct()\n",
    "n_records_after_removing_duplicates = df.count()\n",
    "print('Number of records befor duplicate removal: ', n_records_before_cleaning)\n",
    "print('Number of records after duplicate removal: ', n_records_after_removing_duplicates)\n",
    "print('Number of duplicate records removed: ', n_records_before_cleaning-n_records_after_removing_duplicates)\n",
    "# Missing Value Imputation\n",
    "n_null_records = df.filter('CustomerID is null').count()\n",
    "print('Number of Null records: ', n_null_records)\n",
    "print('Number of records befor missing value imputation: ', df.count())\n",
    "df = df.na.drop()\n",
    "print('Number of records after missing value imputation: ', df.count())\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calcuate Monetary Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- Price: float (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Price\", df.Price.cast('float'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---+\n",
      "|customerID|          Monetary| id|\n",
      "+----------+------------------+---+\n",
      "|     14911|  70473.0999276936|  1|\n",
      "|     14096| 41376.32996287942|  2|\n",
      "|     15098| 40278.89999961853|  3|\n",
      "|     14063| 39920.94912147522|  4|\n",
      "|     14156|36397.009877726436|  5|\n",
      "|     17841| 34582.22974572331|  6|\n",
      "|     15760|  33628.5498046875|  7|\n",
      "|     12918|           32860.5|  8|\n",
      "|     12744| 25481.40001243353|  9|\n",
      "|     17399|    25111.08984375| 10|\n",
      "+----------+------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monDF = df.groupBy('customerID').agg(sum('Price').alias('Monetary'))\n",
    "monDF = monDF.orderBy(desc('Monetary'))\n",
    "tmp= monDF.withColumn(\"new_column\",lit(\"ABC\"))\n",
    "w = Window().partitionBy('new_column').orderBy(lit('A'))\n",
    "monDF = tmp.withColumn(\"id\", row_number().over(w)).drop(\"new_column\")\n",
    "\n",
    "monDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(pair):\n",
    "    (key, val) = pair\n",
    "    result = 0\n",
    "    if val < 0.15*cnt: \n",
    "        result = 1\n",
    "    if 0.15*cnt < val < 0.3*cnt: \n",
    "        result = 2\n",
    "    if 0.3*cnt < val < 0.6*cnt: \n",
    "        result = 3\n",
    "    if 0.6*cnt < val : \n",
    "        result = 4\n",
    "    return (key, result)\n",
    "\n",
    "cnt = monDF.count()\n",
    "monRDD = monDF.rdd\n",
    "monRDD = monRDD.map(lambda x:(x[0],x[2])).map(func)\n",
    "# monRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|customerID|Monetary|\n",
      "+----------+--------+\n",
      "|     14911|       1|\n",
      "|     14096|       1|\n",
      "|     15098|       1|\n",
      "|     14063|       1|\n",
      "|     14156|       1|\n",
      "|     17841|       1|\n",
      "|     15760|       1|\n",
      "|     12918|       1|\n",
      "|     12744|       1|\n",
      "|     17399|       1|\n",
      "+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD code to compute aggregate average\n",
    "monDF = monRDD.toDF([\"customerID\", \"Monetary\"])\n",
    "monDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calcuate Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|customerID|Frequency| id|\n",
      "+----------+---------+---+\n",
      "|     17841|    12638|  1|\n",
      "|     14911|    11444|  2|\n",
      "|     12748|     6662|  3|\n",
      "|     14606|     6500|  4|\n",
      "|     14096|     5128|  5|\n",
      "|     15311|     4579|  6|\n",
      "|     14156|     4118|  7|\n",
      "|     14646|     3890|  8|\n",
      "|     13089|     3391|  9|\n",
      "|     16549|     3098| 10|\n",
      "+----------+---------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frqDF = df.groupBy('customerID').agg(count('invoice').alias('Frequency'))\n",
    "frqDF = frqDF.orderBy(desc('Frequency'))\n",
    "tmp= frqDF.withColumn(\"new_column\",lit(\"ABC\"))\n",
    "w = Window().partitionBy('new_column').orderBy(lit('A'))\n",
    "frqDF = tmp.withColumn(\"id\", row_number().over(w)).drop(\"new_column\")\n",
    "\n",
    "frqDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = frqDF.count()\n",
    "frqRDD = frqDF.rdd\n",
    "frqRDD = frqRDD.map(lambda x:(x[0],x[2])).map(func)\n",
    "# frqRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|customerID|Frequency|\n",
      "+----------+---------+\n",
      "|     17841|        1|\n",
      "|     14911|        1|\n",
      "|     12748|        1|\n",
      "|     14606|        1|\n",
      "|     14096|        1|\n",
      "|     15311|        1|\n",
      "|     14156|        1|\n",
      "|     14646|        1|\n",
      "|     13089|        1|\n",
      "|     16549|        1|\n",
      "+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD code to compute aggregate average\n",
    "frqDF = frqRDD.toDF([\"customerID\", \"Frequency\"])\n",
    "frqDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calcuate Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|CustomerID|   Recency|\n",
      "+----------+----------+\n",
      "|     12713|2011-12-09|\n",
      "|     12985|2011-12-09|\n",
      "|     13298|2011-12-08|\n",
      "|     14251|2011-12-08|\n",
      "|     14138|2011-12-08|\n",
      "|     15877|2011-12-08|\n",
      "|     15520|2011-12-08|\n",
      "|     16322|2011-12-08|\n",
      "|     15156|2011-12-08|\n",
      "|     13521|2011-12-08|\n",
      "+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting an user define function:\n",
    "# This function converts the string cell into a date:\n",
    "dateFormat =  udf (lambda x: dt.strptime(x, '%m/%d/%Y %H:%M'), DateType())\n",
    "\n",
    "recDF = df.groupBy('CustomerID').agg(max('InvoiceDate').alias('Recency'))\n",
    "recDF = recDF.select('CustomerID','Recency') \\\n",
    "        .withColumn('Recency', dateFormat(col('Recency')))\n",
    "recDF = recDF.orderBy(desc('Recency'))\n",
    "recDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compDate(pair):\n",
    "    (key, val) = pair\n",
    "    result = 0\n",
    "    if dt.date(dt(2011, 11, 15)) <= val: \n",
    "        result = 1\n",
    "    if dt.date(dt(2011, 9, 5)) <= val <= dt.date(dt(2011, 11, 14)): \n",
    "        result = 2\n",
    "    if dt.date(dt(2011, 1, 5)) <= val <= dt.date(dt(2011, 9, 4)): \n",
    "        result = 3\n",
    "    if val <= dt.date(dt(2011, 1, 4)): \n",
    "        result = 4\n",
    "    return (key, result)\n",
    "\n",
    "\n",
    "# func =  udf (compDate, DateType())\n",
    "# recDF.withColumn('Recency', func(col('Recency'))).show(10)\n",
    "\n",
    "recRDD = recDF.rdd\n",
    "recRDD = recRDD.map(lambda x:(x[0],x[1])).map(compDate)\n",
    "# recRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|customerID|Recency|\n",
      "+----------+-------+\n",
      "|     12713|      1|\n",
      "|     12985|      1|\n",
      "|     16322|      1|\n",
      "|     15156|      1|\n",
      "|     13471|      1|\n",
      "|     13521|      1|\n",
      "|     15877|      1|\n",
      "|     15520|      1|\n",
      "|     14251|      1|\n",
      "|     14138|      1|\n",
      "+----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD code to compute aggregate average\n",
    "recDF = recRDD.toDF([\"customerID\", \"Recency\"])\n",
    "recDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Number of customers in each category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------+\n",
      "|customerID|Recency|Frequency|Monetary|\n",
      "+----------+-------+---------+--------+\n",
      "|     12394|      3|        4|       4|\n",
      "|     12529|      4|        4|       4|\n",
      "|     12847|      1|        2|       3|\n",
      "|     13192|      2|        2|       2|\n",
      "|     13282|      2|        3|       3|\n",
      "|     13442|      4|        3|       3|\n",
      "|     13610|      4|        1|       1|\n",
      "|     13772|      3|        1|       1|\n",
      "|     13865|      3|        4|       4|\n",
      "|     14157|      4|        3|       3|\n",
      "+----------+-------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uDF = recDF.join(frqDF, on=['CustomerID'], how='outer')\n",
    "uDF = uDF.join(monDF, on=['CustomerID'], how='outer')\n",
    "uDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestCustomer = uDF.where((col('Recency')==lit('1')) \\\n",
    "            & (col('Frequency')==lit('1')) \\\n",
    "            & (col('Monetary')==lit('1'))).count()\n",
    "\n",
    "LoyalCustomer = uDF.where((col('Frequency')==lit('1'))).count()\n",
    "\n",
    "BigSpender = uDF.where((col('Monetary')==lit('1'))).count()\n",
    "\n",
    "AlmostLost = uDF.where((col('Recency')==lit('3')) \\\n",
    "            & (col('Frequency')==lit('1')) \\\n",
    "            & (col('Monetary')==lit('1'))).count()\n",
    "\n",
    "LostCustomers = uDF.where((col('Recency')==lit('4')) \\\n",
    "            & (col('Frequency')==lit('1')) \\\n",
    "            & (col('Monetary')==lit('1'))).count()\n",
    "\n",
    "LostCheapCustomers = uDF.where((col('Recency')==lit('4')) \\\n",
    "            & (col('Frequency')==lit('4')) \\\n",
    "            & (col('Monetary')==lit('4'))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Best Customers:  6\n",
      "Number of Loyal Customers:  891\n",
      "Number of Big Spenders:  891\n",
      "Number of Almost Lost:  103\n",
      "Number of Lost Customers:  371\n",
      "Number of Lost Cheap Customers:  1150\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Best Customers: \", BestCustomer)\n",
    "print(\"Number of Loyal Customers: \", LoyalCustomer)\n",
    "print(\"Number of Big Spenders: \", BigSpender)\n",
    "print(\"Number of Almost Lost: \", AlmostLost)\n",
    "print(\"Number of Lost Customers: \", LostCustomers)\n",
    "print(\"Number of Lost Cheap Customers: \", LostCheapCustomers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
