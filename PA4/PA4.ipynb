{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import sys\n",
    "import datetime\n",
    "import time \n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as sFuncs\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "In case you are using a single-node cluster, executing this cell is essential, as otherwise, SparkContext put the sc.master on 'yarn' which you don't have. The result would be you'll never see a collect() to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"QA4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3 Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option('header','true').load('gs://datathinks-home/online_retail_II.csv')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select\\rename columns\n",
    "df = df.withColumn(\"CustomerID\", df[\"Customer ID\"]).drop(\"Customer ID\", \"Country\", \"Description\")\n",
    "n_records_before_cleaning = df.count()\n",
    "# remove duplicate records\n",
    "df = df.distinct()\n",
    "n_records_after_removing_duplicates = df.count()\n",
    "print('Number of records befor duplicate removal: ', n_records_before_cleaning)\n",
    "print('Number of records after duplicate removal: ', n_records_after_removing_duplicates)\n",
    "print('Number of duplicate records removed: ', n_records_before_cleaning-n_records_after_removing_duplicates)\n",
    "# Missing Value Imputation\n",
    "n_null_records = df.filter('CustomerID is null').count()\n",
    "print('Number of Null records: ', n_null_records)\n",
    "print('Number of records befor missing value imputation: ', df.count())\n",
    "df = df.na.drop()\n",
    "print('Number of records after missing value imputation: ', df.count())\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calcuate Monetary Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Price\", df.Price.cast('float'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monDF = df.groupBy('customerID').agg(sum('Price').alias('Monetary'))\n",
    "monDF = monDF.orderBy(desc('Monetary'))\n",
    "tmp= monDF.withColumn(\"new_column\",lit(\"ABC\"))\n",
    "w = Window().partitionBy('new_column').orderBy(lit('A'))\n",
    "monDF = tmp.withColumn(\"id\", row_number().over(w)).drop(\"new_column\")\n",
    "\n",
    "monDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(pair):\n",
    "    (key, val) = pair\n",
    "    result = 0\n",
    "    if val < 0.15*cnt: \n",
    "        result = 1\n",
    "    if 0.15*cnt < val < 0.3*cnt: \n",
    "        result = 2\n",
    "    if 0.3*cnt < val < 0.6*cnt: \n",
    "        result = 3\n",
    "    if 0.6*cnt < val : \n",
    "        result = 4\n",
    "    return (key, result)\n",
    "\n",
    "cnt = monDF.count()\n",
    "monRDD = monDF.rdd\n",
    "monRDD = monRDD.map(lambda x:(x[0],x[2])).map(func)\n",
    "# monRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD code to compute aggregate average\n",
    "monDF = monRDD.toDF([\"customerID\", \"Monetary\"])\n",
    "monDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calcuate Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frqDF = df.groupBy('customerID').agg(count('invoice').alias('Frequency'))\n",
    "frqDF = frqDF.orderBy(desc('Frequency'))\n",
    "tmp= frqDF.withColumn(\"new_column\",lit(\"ABC\"))\n",
    "w = Window().partitionBy('new_column').orderBy(lit('A'))\n",
    "frqDF = tmp.withColumn(\"id\", row_number().over(w)).drop(\"new_column\")\n",
    "\n",
    "frqDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = frqDF.count()\n",
    "frqRDD = frqDF.rdd\n",
    "frqRDD = frqRDD.map(lambda x:(x[0],x[2])).map(func)\n",
    "# frqRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD code to compute aggregate average\n",
    "frqDF = frqRDD.toDF([\"customerID\", \"Frequency\"])\n",
    "frqDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calcuate Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting an user define function:\n",
    "# This function converts the string cell into a date:\n",
    "dateFormat =  udf (lambda x: dt.strptime(x, '%m/%d/%Y %H:%M'), DateType())\n",
    "\n",
    "recDF = df.groupBy('CustomerID').agg(max('InvoiceDate').alias('Recency'))\n",
    "recDF = recDF.select('CustomerID','Recency') \\\n",
    "        .withColumn('Recency', dateFormat(col('Recency')))\n",
    "recDF = recDF.orderBy(desc('Recency'))\n",
    "recDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compDate(pair):\n",
    "    (key, val) = pair\n",
    "    result = 0\n",
    "    if dt.date(dt(2011, 11, 15)) <= val: \n",
    "        result = 1\n",
    "    if dt.date(dt(2011, 9, 5)) <= val <= dt.date(dt(2011, 11, 14)): \n",
    "        result = 2\n",
    "    if dt.date(dt(2011, 1, 5)) <= val <= dt.date(dt(2011, 9, 4)): \n",
    "        result = 3\n",
    "    if val <= dt.date(dt(2011, 1, 4)): \n",
    "        result = 4\n",
    "    return (key, result)\n",
    "\n",
    "\n",
    "# func =  udf (compDate, DateType())\n",
    "# recDF.withColumn('Recency', func(col('Recency'))).show(10)\n",
    "\n",
    "recRDD = recDF.rdd\n",
    "recRDD = recRDD.map(lambda x:(x[0],x[1])).map(compDate)\n",
    "# recRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD code to compute aggregate average\n",
    "recDF = recRDD.toDF([\"customerID\", \"Recency\"])\n",
    "recDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Number of customers in each category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uDF = recDF.join(frqDF, on=['CustomerID'], how='outer')\n",
    "uDF = uDF.join(monDF, on=['CustomerID'], how='outer')\n",
    "uDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestCustomer = uDF.where((col('Recency')==lit('1')) \\\n",
    "            & (col('Frequency')==lit('1')) \\\n",
    "            & (col('Monetary')==lit('1'))).count()\n",
    "\n",
    "LoyalCustomer = uDF.where((col('Frequency')==lit('1'))).count()\n",
    "\n",
    "BigSpender = uDF.where((col('Monetary')==lit('1'))).count()\n",
    "\n",
    "AlmostLost = uDF.where((col('Recency')==lit('3')) \\\n",
    "            & (col('Frequency')==lit('1')) \\\n",
    "            & (col('Monetary')==lit('1'))).count()\n",
    "\n",
    "LostCustomers = uDF.where((col('Recency')==lit('4')) \\\n",
    "            & (col('Frequency')==lit('1')) \\\n",
    "            & (col('Monetary')==lit('1'))).count()\n",
    "\n",
    "LostCheapCustomers = uDF.where((col('Recency')==lit('4')) \\\n",
    "            & (col('Frequency')==lit('4')) \\\n",
    "            & (col('Monetary')==lit('4'))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of Best Customers: \", BestCustomer)\n",
    "print(\"Number of Loyal Customers: \", LoyalCustomer)\n",
    "print(\"Number of Big Spenders: \", BigSpender)\n",
    "print(\"Number of Almost Lost: \", AlmostLost)\n",
    "print(\"Number of Lost Customers: \", LostCustomers)\n",
    "print(\"Number of Lost Cheap Customers: \", LostCheapCustomers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## PA4 Question 4\n",
    "\n",
    "A freshly-loaded copy of the NYT covid dataset is available as [gs://datathinks-home/covid2.json](gs://datathinks-home/covid2.json).\n",
    "\n",
    "Please donâ€™t upload your data! Instead, starting on March 1, 2020, for the first day of each month, which county had the worst numbers of confirmed cases, and deaths? They might not be the same county. In other words, develop a table that looks like this:\n",
    "\n",
    "| Query | 4/1 | 5/1 | ...etc... |\n",
    "| --- | --- | --- | --- |\n",
    "| Confirmed Cases | nnn, County, State | nnn, County, State |   |\n",
    "| Deaths | nnn, County, State | nnn, County, State |   |\n",
    "\n",
    "This analysis should be done on Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 Question 1\n",
    "\n",
    "The last 5 presidents in our speeches collection were `reagan, bush, clinton, gwbush` and `obama` (that's how the tar.gz files of their speeches are named in Canvas).\n",
    "\n",
    "Pairwise comparisons of the similarities in their speech collections (10 pairs) will give us a half-matrix like shown below (The symmetry in the problem formulation makes it unnecessary to compute the blank spots in the matrix):\n",
    "\n",
    "|  | r | b | c | g | o |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| reagan |   | * | * | * | * |\n",
    "| bush |   |   | * | * | * |\n",
    "| clinton |   |   |   | * | * |\n",
    "| gwbush |   |   |   |   | * |\n",
    "\n",
    "Compute the similarity denoted by each asterisk and answer:\n",
    "\n",
    "1. Using n-gram character shingles, assuming n=4, which two presidents' speeches were the most similar and which were the least similar?\n",
    "2. Using n-gram word shingles, assuming n=3, which two presidents' speeches were the most similar and which were the least similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 Question 3\n",
    "\n",
    "This question builds on the [UCI Online Retail II dataset](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II) analysis you performed in Quiz 3. This time, however, the R, F, M values should be calculated as follows:\n",
    "\n",
    "Recency should be the number of days relative to year-end 2011 (Dec 31). \n",
    "Frequency should simply be the number of transactions in the total period.\n",
    "Monetary value should be the log10 of the total dollars spent. Why log10? We use logs to flatten the range â€” so high-spenders don't skew the analysis.\n",
    "After calculating RFM values as specified above, run K-means clustering to divide the customers into 6 clusters. How do the number of customers in these 6 clusters compare with the clusters you in Question 2 of Quiz 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 Question 4\n",
    "\n",
    "A freshly-loaded copy of the NYT covid dataset is available as [`gs://datathinks-home/covid2.json`](gs://datathinks-home/covid2.json).\n",
    "\n",
    "Please donâ€™t upload your data! Instead, starting on March 1, 2020, for the first day of each month, which county had the worst numbers of confirmed cases, and deaths? They might not be the same county. In other words, develop a table that looks like this:\n",
    "\n",
    "| Query | 4/1 | 5/1 | ...etc... |\n",
    "| --- | --- | --- | --- |\n",
    "| Confirmed Cases | nnn, County, State | nnn, County, State |   |\n",
    "| Deaths | nnn, County, State | nnn, County, State |   |\n",
    "\n",
    "This analysis should be done on Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read the .json file\n",
    "covidDF = spark.read.json('gs://datathinks-home/covid2.json')\n",
    "# covidDF = spark.read.load('gs://datathinks-home/covid2.json', \n",
    "#                           format='json', inferSchema='true')#, header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covidDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covidDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is no missing value in columns other than county_fips_code column which we can drop\n",
    "for col in covidDF.columns:\n",
    "    condition = '{} is null'.format(col)\n",
    "    print('number of nulls in {:s}: {:>10}'.format(col, covidDF.filter(condition).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to \n",
    "covidDF = covidDF.withColumn('confirmed_cases', covidDF.confirmed_cases.cast('int'))\\\n",
    "                 .withColumn('deaths', covidDF.deaths.cast('int'))\\\n",
    "                 .withColumn('date', to_date(covidDF.date))\n",
    "\n",
    "dateDF = covidDF.filter(sFuncs.dayofmonth(covidDF.date) == 1).sort('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findStat(num, county, state):\n",
    "    return str(num) + ', ' + str(county) + ', ' + str(state)\n",
    "\n",
    "# def findStat(*row):\n",
    "#     return ','.join(row)\n",
    "\n",
    "func = sFuncs.udf(findStat)\n",
    "\n",
    "start = time.time()\n",
    "max_cases = dateDF.sort(desc('confirmed_cases'))\\\n",
    "                  .groupBy('date')\\\n",
    "                  .agg(sFuncs.max('confirmed_cases').alias('cases'), \\\n",
    "                       first('county').alias('county'), \\\n",
    "                       first('state_name').alias('state'))\\\n",
    "                  .select(sFuncs.date_format('date', 'MM/dd').alias('date'), \\\n",
    "                          func('cases', 'county', 'state').alias('max_cases'))\n",
    "\n",
    "max_deaths = dateDF.sort(desc('deaths'))\\\n",
    "                   .groupBy('date')\\\n",
    "                   .agg(sFuncs.max('deaths').alias('deaths'), \\\n",
    "                        first('county').alias('county'), \\\n",
    "                        first('state_name').alias('state'))\\\n",
    "                   .select(sFuncs.date_format('date', 'MM/dd').alias('date'), \\\n",
    "                           func('deaths', 'county', 'state').alias('max_deaths'))\n",
    "\n",
    "max_cases.join(max_deaths, ['date']).sort('date').show(truncate=False)\n",
    "\n",
    "print('elapsed time: {:.0f} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byDate = Window.partitionBy('date').orderBy(desc('deaths'))\n",
    "dateDF.withColumn('rank', dense_rank().over(byDate)).filter('rank == 1').show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
