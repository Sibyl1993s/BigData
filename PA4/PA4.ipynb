{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import sys\n",
    "import datetime\n",
    "import time \n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as sFuncs\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# from pyspark.mllib.clustering import KMeans, KMeansModel\n",
    "# from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "In case you are using a single-node cluster, executing this cell is essential, as otherwise, SparkContext put the sc.master on 'yarn' which you don't have. The result would be you'll never see a collect() to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc._conf.getAll()\n",
    "sc.stop()\n",
    "conf = SparkConf().setMaster('local[4]')\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2)]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 Question 1\n",
    "\n",
    "The last 5 presidents in our speeches collection were `reagan, bush, clinton, gwbush` and `obama` (that's how the tar.gz files of their speeches are named in Canvas).\n",
    "\n",
    "Pairwise comparisons of the similarities in their speech collections (10 pairs) will give us a half-matrix like shown below (The symmetry in the problem formulation makes it unnecessary to compute the blank spots in the matrix):\n",
    "\n",
    "|  | r | b | c | g | o |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| reagan |   | * | * | * | * |\n",
    "| bush |   |   | * | * | * |\n",
    "| clinton |   |   |   | * | * |\n",
    "| gwbush |   |   |   |   | * |\n",
    "\n",
    "Compute the similarity denoted by each asterisk and answer:\n",
    "\n",
    "1. Using n-gram character shingles, assuming n=4, which two presidents' speeches were the most similar and which were the least similar?\n",
    "2. Using n-gram word shingles, assuming n=3, which two presidents' speeches were the most similar and which were the least similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to unpack the dataset into the current directory and then move it to gs_bucket\n",
    "# NOTE that this cell needs to run once\n",
    "%%bash\n",
    "cd /home/BigData/PA4\n",
    "tar -xcvf ./dataset.tar.gz \n",
    "gsutil -q cp -rq ./dataset/* gs://saberbf0098/presidential_speech_corpus/\n",
    "\n",
    "gsutil ls -al gs://saberbf0098/presidential_speech_corpus/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the documents for each president into a separate rdd\n",
    "presidents = ['reagan', 'bush', 'clinton', 'gwbush', 'obama']\n",
    "speech = {p:sc.textFile('gs://saberbf0098/presidential_speech_corpus/{0}/{0}_speeches_*.txt'\\\n",
    "                     .format(p)) for p in presidents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create n-gram character shingles\n",
    "def ngramsC(phrase, N):\n",
    "    grams = [phrase.lower()[i:i+N] for i in range(len(phrase)-N+1)]\n",
    "    return grams#set(grams)\n",
    "\n",
    "def cmp_matrix(set1, set2):\n",
    "    return float(len(set1&set2))/float(len(set1|set2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (reagan,obama) = 0.0060\n"
     ]
    }
   ],
   "source": [
    "# To test to see if the algorithm works fine, check the similarity between 'test' \n",
    "# and 'obama' documents. This should give us a near zero result. Also, a test between 'obama'\n",
    "# and 'obama' should give 1.0\n",
    "set1 = sc.textFile('gs://saberbf0098/test.txt').flatMap(lambda x: ngramsC(x,N)).collect()\n",
    "set2 = speech['obama'].flatMap(lambda x: ngramsC(x,N)).collect()\n",
    "print (\"Similarity ({},{}) = {:.4f}\".format('test','obama',cmp_matrix(set(set2), set(set1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-gram character shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (reagan,obama) = 0.5601\n",
      "Similarity (reagan,gwbush) = 0.5314\n",
      "Similarity (reagan,clinton) = 0.5590\n",
      "Similarity (reagan,bush) = 0.5461\n",
      "Similarity (bush,obama) = 0.5580\n",
      "Similarity (bush,gwbush) = 0.5684\n",
      "Similarity (bush,clinton) = 0.5774\n",
      "Similarity (clinton,obama) = 0.5788\n",
      "Similarity (clinton,gwbush) = 0.5755\n",
      "Similarity (gwbush,obama) = 0.5777\n"
     ]
    }
   ],
   "source": [
    "# working on 4-gram characters\n",
    "presidents = ['reagan', 'bush', 'clinton', 'gwbush', 'obama']\n",
    "presidents = list(reversed(presidents))\n",
    "cmp_4gram=dict()\n",
    "N=4\n",
    "while presidents:\n",
    "    p = presidents.pop()\n",
    "    set1 = speech[p].flatMap(lambda x: ngramsC(x,N)).collect()\n",
    "    for q in presidents:\n",
    "        set2 = speech[q].flatMap(lambda x: ngramsC(x,N)).collect()\n",
    "        cmp_4gram[(p,q)] = cmp_matrix(set(set1), set(set2))\n",
    "        print (\"Similarity ({},{}) = {:.4f}\".format(p,q,cmp_4gram[(p,q)]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least similarity btween ('reagan', 'gwbush') speeches.\n",
      "Most similarity btween ('clinton', 'obama') speeches.\n"
     ]
    }
   ],
   "source": [
    "print('Least similarity btween {} speeches.'.format(min(cmp_4gram, key=cmp_4gram.get)))\n",
    "print('Most similarity btween {} speeches.'.format(max(cmp_4gram, key=cmp_4gram.get)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-gram character shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (reagan,obama) = 0.1311\n",
      "Similarity (reagan,gwbush) = 0.1141\n",
      "Similarity (reagan,clinton) = 0.1295\n",
      "Similarity (reagan,bush) = 0.1175\n",
      "Similarity (bush,obama) = 0.1141\n",
      "Similarity (bush,gwbush) = 0.1133\n",
      "Similarity (bush,clinton) = 0.1240\n",
      "Similarity (clinton,obama) = 0.1386\n",
      "Similarity (clinton,gwbush) = 0.1222\n",
      "Similarity (gwbush,obama) = 0.1251\n"
     ]
    }
   ],
   "source": [
    "# working on 10-gram characters\n",
    "presidents = ['reagan', 'bush', 'clinton', 'gwbush', 'obama']\n",
    "presidents = list(reversed(presidents))\n",
    "cmp_10gram=dict()\n",
    "N=10\n",
    "while presidents:\n",
    "    p = presidents.pop()\n",
    "    set1 = speech[p].flatMap(lambda x: ngramsC(x,N)).collect()\n",
    "    for q in presidents:\n",
    "        set2 = speech[q].flatMap(lambda x: ngramsC(x,N)).collect()\n",
    "        cmp_10gram[(p,q)] = cmp_matrix(set(set1), set(set2))\n",
    "        print (\"Similarity ({},{}) = {:.4f}\".format(p,q,cmp_10gram[(p,q)]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least similarity btween ('bush', 'gwbush') speeches.\n",
      "Most similarity btween ('clinton', 'obama') speeches.\n"
     ]
    }
   ],
   "source": [
    "print('Least similarity btween {} speeches.'.format(min(cmp_10gram, key=cmp_10gram.get)))\n",
    "print('Most similarity btween {} speeches.'.format(max(cmp_10gram, key=cmp_10gram.get)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-gram word shignles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to create n-gram word shingles\n",
    "def ngramsW(phrase, N):\n",
    "    grams = [tuple(phrase[i:i+N]) for i in range(len(phrase)-N+1)]\n",
    "    return grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (test,obama) = 0.0000\n"
     ]
    }
   ],
   "source": [
    "# To test to see if the algorithm works fine, check the similarity between 'test' \n",
    "# and 'obama' documents. This should give us a near zero result. Also, a test between 'obama'\n",
    "# and 'obama' should give 1.0\n",
    "set1 = sc.textFile('gs://saberbf0098/test.txt').map(lambda x:x.split(' '))\\\n",
    "         .flatMap(lambda x: ngramsW(x,N)).collect()\n",
    "set2 = speech['obama'].map(lambda x:x.split(' '))\\\n",
    "                      .flatMap(lambda x: ngramsW(x,N)).collect()\n",
    "print (\"Similarity ({},{}) = {:.4f}\".format('test','obama',cmp_matrix(set(set1), set(set2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity (reagan,obama) = 0.0405\n",
      "Similarity (reagan,gwbush) = 0.0322\n",
      "Similarity (reagan,clinton) = 0.0417\n",
      "Similarity (reagan,bush) = 0.0375\n",
      "Similarity (bush,obama) = 0.0356\n",
      "Similarity (bush,gwbush) = 0.0334\n",
      "Similarity (bush,clinton) = 0.0411\n",
      "Similarity (clinton,obama) = 0.0464\n",
      "Similarity (clinton,gwbush) = 0.0376\n",
      "Similarity (gwbush,obama) = 0.0385\n"
     ]
    }
   ],
   "source": [
    "presidents = ['reagan', 'bush', 'clinton', 'gwbush', 'obama']\n",
    "presidents = list(reversed(presidents))\n",
    "cmp_3gram=dict()\n",
    "N = 3\n",
    "while presidents:\n",
    "    p = presidents.pop()\n",
    "    set1 = speech[p].map(lambda x:x.split(' ')).flatMap(lambda x: ngramsW(x,N)).collect()\n",
    "    for q in presidents:\n",
    "        set2 = speech[q].map(lambda x:x.split(' ')).flatMap(lambda x: ngramsW(x,N)).collect()\n",
    "        cmp_3gram[(p,q)] = cmp_matrix(set(set1), set(set2))\n",
    "        print (\"Similarity ({},{}) = {:.4f}\".format(p,q,cmp_3gram[(p,q)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least similarity btween ('reagan', 'gwbush') spechees.\n",
      "Most similarity btween ('clinton', 'obama') spechees.\n"
     ]
    }
   ],
   "source": [
    "print('Least similarity btween {} spechees.'.format(min(cmp_3gram, key=cmp_3gram.get)))\n",
    "print('Most similarity btween {} spechees.'.format(max(cmp_3gram, key=cmp_3gram.get)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notice - this section was used to test if everything can be done without rdds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without using rdds\n",
    "test = sc.textFile('gs://saberbf0098/test.txt').reduce(lambda x,y:x+' '+y)\n",
    "ngramsC(test,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presidents = ['reagan', 'bush', 'clinton', 'gwbush', 'obama']\n",
    "presidents = list(reversed(presidents))\n",
    "cmp_4gram=dict()\n",
    "N=4\n",
    "while presidents:\n",
    "    p = presidents.pop()\n",
    "    set1 = ngramsC(speech[p],N)\n",
    "    for q in presidents:\n",
    "        set2 = ngramsC(speech[q],N)\n",
    "        cmp_4gram[(p,q)] = cmp_matrix(set(set1), set(set2))\n",
    "        print (\"Similarity ({},{}) = {:.4f}\".format(p,q,cmp_4gram[(p,q)]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('obama', 0.5600765337014701),\n",
       " ('gwbush', 0.5314372804298408),\n",
       " ('clinton', 0.5590224135747752),\n",
       " ('bush', 0.5460797438118487),\n",
       " ('obama', 0.55795140650544),\n",
       " ('gwbush', 0.5683723257850619),\n",
       " ('clinton', 0.5773999398134216),\n",
       " ('obama', 0.5787752432344044),\n",
       " ('gwbush', 0.5755378509832005),\n",
       " ('obama', 0.5776688020862861)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.master('local[4]').getOrCreate()\n",
    "lst = [(k[1],v) for (k,v) in list(cmp_4gram.items())]\n",
    "rdd = sc.parallelize(lst)#.reduce(lambda x:x)\n",
    "rdd.collect()\n",
    "# df = spark.createDataFrame(lst)\n",
    "# df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 Question 3\n",
    "\n",
    "This question builds on the [UCI Online Retail II dataset](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II) analysis you performed in Quiz 3. This time, however, the R, F, M values should be calculated as follows:\n",
    "\n",
    "- Recency should be the number of days relative to year-end 2011 (Dec 31). \n",
    "- Frequency should simply be the number of transactions in the total period.\n",
    "- Monetary value should be the log10 of the total dollars spent. Why log10? We use logs to flatten the range — so high-spenders don't skew the analysis.\n",
    "\n",
    "After calculating RFM values as specified above, run K-means clustering to divide the customers into 6 clusters. How do the number of customers in these 6 clusters compare with the clusters you in Question 2 of Quiz 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "|Invoice|StockCode|         Description|Quantity|   InvoiceDate|Price|Customer ID|       Country|\n",
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "| 489434|    85048|15CM CHRISTMAS GL...|      12|12/1/2009 7:45| 6.95|      13085|United Kingdom|\n",
      "| 489434|   79323P|  PINK CHERRY LIGHTS|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|\n",
      "| 489434|   79323W| WHITE CHERRY LIGHTS|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|\n",
      "| 489434|    22041|\"RECORD FRAME 7\"\"...|      48|12/1/2009 7:45|  2.1|      13085|United Kingdom|\n",
      "| 489434|    21232|STRAWBERRY CERAMI...|      24|12/1/2009 7:45| 1.25|      13085|United Kingdom|\n",
      "| 489434|    22064|PINK DOUGHNUT TRI...|      24|12/1/2009 7:45| 1.65|      13085|United Kingdom|\n",
      "| 489434|    21871| SAVE THE PLANET MUG|      24|12/1/2009 7:45| 1.25|      13085|United Kingdom|\n",
      "| 489434|    21523|FANCY FONT HOME S...|      10|12/1/2009 7:45| 5.95|      13085|United Kingdom|\n",
      "| 489435|    22350|           CAT BOWL |      12|12/1/2009 7:46| 2.55|      13085|United Kingdom|\n",
      "| 489435|    22349|DOG BOWL , CHASIN...|      12|12/1/2009 7:46| 3.75|      13085|United Kingdom|\n",
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# to load the data into a DataFrame\n",
    "df = spark.read.load('gs://datathinks-home/online_retail_II.csv', \n",
    "                          format='csv', inferSchema=True, header=True)\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22950"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['Quantity'] <=0).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records befor preprocessing:  1067371\n",
      "Number of records after missing value imputation:  824364\n"
     ]
    }
   ],
   "source": [
    "# select\\rename columns\n",
    "n_records_before_cleaning = df.count()\n",
    "print('Number of records befor preprocessing: ', n_records_before_cleaning)\n",
    "# Missing Value Imputation\n",
    "# df = df.na.drop()\n",
    "df = df.dropna()\n",
    "print('Number of records after missing value imputation: ', df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calcuate Monetary Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def totalSpend(quantity, price):\n",
    "    return int(quantity) * float(price)\n",
    "udfTotalSpend = sFuncs.udf(totalSpend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+\n",
      "|customer ID|         Monetary|\n",
      "+-----------+-----------------+\n",
      "|      18102|5.776857458307348|\n",
      "|      14646|5.718785647857314|\n",
      "|      14156|5.472119441193257|\n",
      "|      14911|5.431763340314062|\n",
      "|      17450|5.368434519929445|\n",
      "|      13694|5.280636454508753|\n",
      "|      17511|5.235240454598928|\n",
      "|      12415|5.156153108660705|\n",
      "|      16684| 5.15076334554805|\n",
      "|      15061|5.134787241987954|\n",
      "+-----------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monDF = df.withColumn('totalSpend', udfTotalSpend('Quantity', 'Price'))\\\n",
    "          .groupBy('customer ID').agg(sFuncs.log10(sFuncs.sum('totalSpend')).alias('Monetary'))\\\n",
    "          .dropna()\\\n",
    "          .sort(sFuncs.desc('Monetary'))\n",
    "monDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calcuate Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------+\n",
      "|customer ID|Frequency|\n",
      "+-----------+---------+\n",
      "|      17841|    13097|\n",
      "|      14911|    11613|\n",
      "|      12748|     7307|\n",
      "|      14606|     6709|\n",
      "|      14096|     5128|\n",
      "|      15311|     4717|\n",
      "|      14156|     4130|\n",
      "|      14646|     3890|\n",
      "|      13089|     3438|\n",
      "|      16549|     3255|\n",
      "+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frqDF = df.groupBy('customer ID')\\\n",
    "          .agg(sFuncs.count('invoice').alias('Frequency'))\\\n",
    "          .sort(sFuncs.desc('Frequency'))\n",
    "\n",
    "frqDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calcuate Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+-------+\n",
      "|Customer ID|maxInvoiceDate|Recency|\n",
      "+-----------+--------------+-------+\n",
      "|      17641|    2009-12-01|    760|\n",
      "|      14654|    2009-12-01|    760|\n",
      "|      17056|    2009-12-01|    760|\n",
      "|      12636|    2009-12-01|    760|\n",
      "|      17592|    2009-12-01|    760|\n",
      "|      17485|    2009-12-01|    760|\n",
      "|      13526|    2009-12-01|    760|\n",
      "|      14980|    2009-12-02|    759|\n",
      "|      15833|    2009-12-02|    759|\n",
      "|      17087|    2009-12-02|    759|\n",
      "+-----------+--------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "present = datetime.date(2011, 12, 31)\n",
    "\n",
    "def daysToPresent(date):\n",
    "    return (present - date).days\n",
    "\n",
    "udfDaysToPresent = sFuncs.udf(daysToPresent)\n",
    "\n",
    "# below, we need to use cast() function to make sure Recency column is integer. \n",
    "# Otherwise, it will be string and sorting works based on UTF value.\n",
    "recDF = df.withColumn('InvoiceDate', sFuncs.to_date(df.InvoiceDate,\"MM/dd/yyyy\"))\\\n",
    "          .groupBy('Customer ID')\\\n",
    "          .agg(sFuncs.max('InvoiceDate').alias('maxInvoiceDate'))\\\n",
    "          .withColumn('Recency', udfDaysToPresent('maxInvoiceDate').cast('int'))\\\n",
    "          .sort(sFuncs.desc('Recency'))\n",
    "recDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Number of customers in each category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+------------------+\n",
      "|Customer ID|Recency|Frequency|          Monetary|\n",
      "+-----------+-------+---------+------------------+\n",
      "|      12799|    551|       15| 2.341137638740964|\n",
      "|      12940|     68|      103|  2.94264785566263|\n",
      "|      13285|     45|      231| 3.526932149812256|\n",
      "|      13289|    745|       16| 2.488480208426273|\n",
      "|      13623|     52|      303|3.3885203670539803|\n",
      "|      13832|     39|       58|2.7799137998502625|\n",
      "|      13840|    438|       40|2.8138477542288545|\n",
      "|      14450|    202|       76|3.0524784722647733|\n",
      "|      14570|    302|       68| 2.787991505131025|\n",
      "|      15447|    352|       30| 2.685679052400608|\n",
      "+-----------+-------+---------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfm = recDF.join(frqDF, on=['Customer ID'], how='outer')\\\n",
    "           .join(monDF, on=['Customer ID'], how='outer')\\\n",
    "           .drop('maxInvoiceDate')\\\n",
    "           .dropna()\n",
    "rfm.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+------------------+--------------------------------+\n",
      "|Customer ID|Recency|Frequency|Monetary          |features                        |\n",
      "+-----------+-------+---------+------------------+--------------------------------+\n",
      "|12799      |551    |15       |2.341137638740964 |[551.0,15.0,2.341137638740964]  |\n",
      "|12940      |68     |103      |2.94264785566263  |[68.0,103.0,2.94264785566263]   |\n",
      "|13285      |45     |231      |3.526932149812256 |[45.0,231.0,3.526932149812256]  |\n",
      "|13289      |745    |16       |2.488480208426273 |[745.0,16.0,2.488480208426273]  |\n",
      "|13623      |52     |303      |3.3885203670539803|[52.0,303.0,3.3885203670539803] |\n",
      "|13832      |39     |58       |2.7799137998502625|[39.0,58.0,2.7799137998502625]  |\n",
      "|13840      |438    |40       |2.8138477542288545|[438.0,40.0,2.8138477542288545] |\n",
      "|14450      |202    |76       |3.0524784722647733|[202.0,76.0,3.0524784722647733] |\n",
      "|14570      |302    |68       |2.787991505131025 |[302.0,68.0,2.787991505131025]  |\n",
      "|15447      |352    |30       |2.685679052400608 |[352.0,30.0,2.685679052400608]  |\n",
      "|15619      |32     |3        |2.5268559871258747|[32.0,3.0,2.5268559871258747]   |\n",
      "|15727      |38     |694      |3.9752254121473287|[38.0,694.0,3.9752254121473287] |\n",
      "|15790      |32     |35       |2.34409740359441  |[32.0,35.0,2.34409740359441]    |\n",
      "|15846      |407    |29       |2.0294243640580167|[407.0,29.0,2.0294243640580167] |\n",
      "|15957      |53     |151      |2.9685296443748395|[53.0,151.0,2.9685296443748395] |\n",
      "|16339      |306    |20       |2.041195233696809 |[306.0,20.0,2.041195233696809]  |\n",
      "|16386      |50     |147      |3.042414049303677 |[50.0,147.0,3.042414049303677]  |\n",
      "|16503      |128    |216      |3.5560478813210152|[128.0,216.0,3.5560478813210152]|\n",
      "|16574      |93     |86       |3.114537594939494 |[93.0,86.0,3.114537594939494]   |\n",
      "|16861      |81     |53       |2.9553846484178226|[81.0,53.0,2.9553846484178226]  |\n",
      "+-----------+-------+---------+------------------+--------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step1 - to assemble features in a vector\n",
    "vecAssembler = VectorAssembler(inputCols=[\"Recency\", \"Frequency\", \"Monetary\"],\\\n",
    "                               outputCol=\"features\",\\\n",
    "                               handleInvalid=\"skip\")\n",
    "vecRFM = vecAssembler.transform(rfm)\n",
    "vecRFM.show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step2 - to fit K-means model\n",
    "kmeans = KMeans(k=6, seed=1)\n",
    "model = kmeans.fit(vecRFM.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+---------+------------------+--------------------+----------+\n",
      "|Customer ID|Recency|Frequency|          Monetary|            features|prediction|\n",
      "+-----------+-------+---------+------------------+--------------------+----------+\n",
      "|      12799|    551|       15| 2.341137638740964|[551.0,15.0,2.341...|         0|\n",
      "|      12940|     68|      103|  2.94264785566263|[68.0,103.0,2.942...|         4|\n",
      "|      13285|     45|      231| 3.526932149812256|[45.0,231.0,3.526...|         5|\n",
      "|      13289|    745|       16| 2.488480208426273|[745.0,16.0,2.488...|         0|\n",
      "|      13623|     52|      303|3.3885203670539803|[52.0,303.0,3.388...|         5|\n",
      "|      13832|     39|       58|2.7799137998502625|[39.0,58.0,2.7799...|         4|\n",
      "|      13840|    438|       40|2.8138477542288545|[438.0,40.0,2.813...|         0|\n",
      "|      14450|    202|       76|3.0524784722647733|[202.0,76.0,3.052...|         4|\n",
      "|      14570|    302|       68| 2.787991505131025|[302.0,68.0,2.787...|         0|\n",
      "|      15447|    352|       30| 2.685679052400608|[352.0,30.0,2.685...|         0|\n",
      "|      15619|     32|        3|2.5268559871258747|[32.0,3.0,2.52685...|         4|\n",
      "|      15727|     38|      694|3.9752254121473287|[38.0,694.0,3.975...|         5|\n",
      "|      15790|     32|       35|  2.34409740359441|[32.0,35.0,2.3440...|         4|\n",
      "|      15846|    407|       29|2.0294243640580167|[407.0,29.0,2.029...|         0|\n",
      "|      15957|     53|      151|2.9685296443748395|[53.0,151.0,2.968...|         4|\n",
      "|      16339|    306|       20| 2.041195233696809|[306.0,20.0,2.041...|         0|\n",
      "|      16386|     50|      147| 3.042414049303677|[50.0,147.0,3.042...|         4|\n",
      "|      16503|    128|      216|3.5560478813210152|[128.0,216.0,3.55...|         4|\n",
      "|      16574|     93|       86| 3.114537594939494|[93.0,86.0,3.1145...|         4|\n",
      "|      16861|     81|       53|2.9553846484178226|[81.0,53.0,2.9553...|         4|\n",
      "+-----------+-------+---------+------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step3 - do the prediction\n",
    "transformed = model.transform(vecRFM)\n",
    "transformed.show(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestCustomer = transformed.where('prediction == 0').count()\n",
    "\n",
    "LoyalCustomer = transformed.where('prediction == 1').count()\n",
    "\n",
    "BigSpender = transformed.where('prediction == 2').count()\n",
    "\n",
    "AlmostLost = transformed.where('prediction == 3').count()\n",
    "\n",
    "LostCustomers = transformed.where('prediction == 4').count()\n",
    "\n",
    "LostCheapCustomers = transformed.where('prediction == 5').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Best Customers:  1838\n",
      "Number of Loyal Customers:  3\n",
      "Number of Big Spenders:  13\n",
      "Number of Almost Lost:  151\n",
      "Number of Lost Customers:  3076\n",
      "Number of Lost Cheap Customers:  762\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Best Customers: \", BestCustomer)\n",
    "print(\"Number of Loyal Customers: \", LoyalCustomer)\n",
    "print(\"Number of Big Spenders: \", BigSpender)\n",
    "print(\"Number of Almost Lost: \", AlmostLost)\n",
    "print(\"Number of Lost Customers: \", LostCustomers)\n",
    "print(\"Number of Lost Cheap Customers: \", LostCheapCustomers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 Question 4\n",
    "\n",
    "A freshly-loaded copy of the NYT covid dataset is available as [`gs://datathinks-home/covid2.json`](gs://datathinks-home/covid2.json).\n",
    "\n",
    "Please don’t upload your data! Instead, starting on March 1, 2020, for the first day of each month, which county had the worst numbers of confirmed cases, and deaths? They might not be the same county. In other words, develop a table that looks like this:\n",
    "\n",
    "| Query | 4/1 | 5/1 | ...etc... |\n",
    "| --- | --- | --- | --- |\n",
    "| Confirmed Cases | nnn, County, State | nnn, County, State |   |\n",
    "| Deaths | nnn, County, State | nnn, County, State |   |\n",
    "\n",
    "This analysis should be done on Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to read the .json file\n",
    "# covidDF = spark.read.json('gs://datathinks-home/covid2.json')\n",
    "covidDF = spark.read.load('gs://datathinks-home/covid2.json', \n",
    "                          format='json', inferSchema='true', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------+----------+------+----------+\n",
      "|confirmed_cases|county     |county_fips_code|date      |deaths|state_name|\n",
      "+---------------+-----------+----------------+----------+------+----------+\n",
      "|12             |Kansas City|null            |2020-03-20|0     |Missouri  |\n",
      "|13             |Kansas City|null            |2020-03-21|0     |Missouri  |\n",
      "|13             |Kansas City|null            |2020-03-22|0     |Missouri  |\n",
      "|18             |Kansas City|null            |2020-03-23|0     |Missouri  |\n",
      "|30             |Kansas City|null            |2020-03-24|0     |Missouri  |\n",
      "|51             |Kansas City|null            |2020-03-25|0     |Missouri  |\n",
      "|64             |Kansas City|null            |2020-03-26|0     |Missouri  |\n",
      "|78             |Kansas City|null            |2020-03-27|0     |Missouri  |\n",
      "|94             |Kansas City|null            |2020-03-28|0     |Missouri  |\n",
      "|102            |Kansas City|null            |2020-03-29|0     |Missouri  |\n",
      "+---------------+-----------+----------------+----------+------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covidDF.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- confirmed_cases: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- county_fips_code: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- deaths: string (nullable = true)\n",
      " |-- state_name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covidDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of nulls in confirmed_cases:          0\n",
      "number of nulls in county:          0\n",
      "number of nulls in county_fips_code:       6691\n",
      "number of nulls in date:          0\n",
      "number of nulls in deaths:          0\n",
      "number of nulls in state_name:          0\n"
     ]
    }
   ],
   "source": [
    "# there is no missing value in columns other than county_fips_code column which we can drop\n",
    "for col in covidDF.columns:\n",
    "    condition = '{} is null'.format(col)\n",
    "    print('number of nulls in {:s}: {:>10}'.format(col, covidDF.filter(condition).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to conver columns to proper datatype \n",
    "covidDF = covidDF.withColumn('confirmed_cases', covidDF.confirmed_cases.cast('int'))\\\n",
    "                 .withColumn('deaths', covidDF.deaths.cast('int'))\\\n",
    "                 .withColumn('date', sFuncs.to_date(covidDF.date))\n",
    "\n",
    "# create a new dataset holding records for the first day of each months\n",
    "dateDF = covidDF.filter(sFuncs.dayofmonth(covidDF.date) == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------+------------------------------+\n",
      "|date |max_cases                      |max_deaths                    |\n",
      "+-----+-------------------------------+------------------------------+\n",
      "|02/01|2, Cook, Illinois              |0, Maricopa, Arizona          |\n",
      "|03/01|13, Douglas, Nebraska          |3, King, Washington           |\n",
      "|04/01|47914, New York City, New York |1848, New York City, New York |\n",
      "|05/01|174931, New York City, New York|17931, New York City, New York|\n",
      "|06/01|208550, New York City, New York|21090, New York City, New York|\n",
      "|07/01|220143, New York City, New York|22574, New York City, New York|\n",
      "|08/01|230147, New York City, New York|23007, New York City, New York|\n",
      "|09/01|242521, Los Angeles, California|23703, New York City, New York|\n",
      "|10/01|271371, Los Angeles, California|23829, New York City, New York|\n",
      "|11/01|309190, Los Angeles, California|24013, New York City, New York|\n",
      "+-----+-------------------------------+------------------------------+\n",
      "\n",
      "elapsed time: 12 seconds\n"
     ]
    }
   ],
   "source": [
    "def findStat(num, county, state):\n",
    "    return str(num) + ', ' + str(county) + ', ' + str(state)\n",
    "\n",
    "# def findStat(*row):\n",
    "#     return ','.join(row)\n",
    "\n",
    "func = sFuncs.udf(findStat)\n",
    "\n",
    "start = time.time()\n",
    "max_cases = dateDF.sort(sFuncs.desc('confirmed_cases'))\\\n",
    "                  .groupBy('date')\\\n",
    "                  .agg(sFuncs.max('confirmed_cases').alias('cases'), \\\n",
    "                       sFuncs.first('county').alias('county'), \\\n",
    "                       sFuncs.first('state_name').alias('state'))\\\n",
    "                  .select(sFuncs.date_format('date', 'MM/dd').alias('date'), \\\n",
    "                          func('cases', 'county', 'state').alias('max_cases'))\n",
    "\n",
    "max_deaths = dateDF.sort(sFuncs.desc('deaths'))\\\n",
    "                   .groupBy('date')\\\n",
    "                   .agg(sFuncs.max('deaths').alias('deaths'), \\\n",
    "                        sFuncs.first('county').alias('county'), \\\n",
    "                        sFuncs.first('state_name').alias('state'))\\\n",
    "                   .select(sFuncs.date_format('date', 'MM/dd').alias('date'), \\\n",
    "                           func('deaths', 'county', 'state').alias('max_deaths'))\n",
    "\n",
    "max_cases.join(max_deaths, ['date']).sort('date').show(truncate=False)\n",
    "\n",
    "print('elapsed time: {:.0f} seconds'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------------+------------------------------+\n",
      "|date |max_cases                      |max_deaths                    |\n",
      "+-----+-------------------------------+------------------------------+\n",
      "|02/01|2, Cook, Illinois              |0, Maricopa, Arizona          |\n",
      "|03/01|13, Douglas, Nebraska          |3, King, Washington           |\n",
      "|04/01|47914, New York City, New York |1848, New York City, New York |\n",
      "|05/01|174931, New York City, New York|17931, New York City, New York|\n",
      "|06/01|208550, New York City, New York|21090, New York City, New York|\n",
      "|07/01|220143, New York City, New York|22574, New York City, New York|\n",
      "|08/01|230147, New York City, New York|23007, New York City, New York|\n",
      "|09/01|242521, Los Angeles, California|23703, New York City, New York|\n",
      "|10/01|271371, Los Angeles, California|23829, New York City, New York|\n",
      "|11/01|309190, Los Angeles, California|24013, New York City, New York|\n",
      "+-----+-------------------------------+------------------------------+\n",
      "\n",
      "elapsed time: 9 seconds\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, this part can be done using window partitioning which is faster than aggregation\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "byCase = Window.partitionBy('date').orderBy(sFuncs.desc('confirmed_cases'))\n",
    "\n",
    "w_max_cases = dateDF.withColumn('rank', sFuncs.dense_rank().over(byCase))\\\n",
    "                     .filter('rank == 1')\\\n",
    "                     .withColumn('rn', sFuncs.row_number().over(byCase)).where('rn==1').drop('rn')\\\n",
    "                     .select(sFuncs.date_format('date', 'MM/dd').alias('date'), \\\n",
    "                             func('confirmed_cases', 'county', 'state_name').alias('max_cases'))\\\n",
    "\n",
    "byDeath = Window.partitionBy('date').orderBy(sFuncs.desc('deaths'))\n",
    "\n",
    "w_max_deaths = dateDF.withColumn('rank', sFuncs.dense_rank().over(byDeath))\\\n",
    "                     .filter('rank == 1')\\\n",
    "                     .withColumn('rn', sFuncs.row_number().over(byDeath)).where('rn==1').drop('rn')\\\n",
    "                     .select(sFuncs.date_format('date', 'MM/dd').alias('date'), \\\n",
    "                             func('deaths', 'county', 'state_name').alias('max_deaths'))\\\n",
    "\n",
    "\n",
    "\n",
    "w_max_cases.join(w_max_deaths, ['date']).sort('date').show(truncate=False)\n",
    "\n",
    "print('elapsed time: {:.0f} seconds'.format(time.time() - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
