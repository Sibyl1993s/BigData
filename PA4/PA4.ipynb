{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "In case you are using a single-node cluster, executing this cell is essential, as otherwise, SparkContext put the sc.master on 'yarn' which you don't have. The result would be you'll never see a collect() to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[4]\") \\\n",
    "    .appName(\"QA4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3 Part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -put /home/saberbf/transition-matrix.txt  /user/root/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['S0', 'S1', 'S2', 'S3', 'S4', 'Sf']\n",
    "links = sc.parallelize(map(lambda s: (s, states), states)) \\\n",
    "# links.collect()\n",
    "\n",
    "ranks = sc.parallelize(states).map(lambda s: (s, 1.0))\n",
    "# ranks.collect()\n",
    "\n",
    "wordProbs = sc.parallelize([('S0',{}), \\\n",
    "                            ('S1', {'Tom':1/5, 'John':1/5, 'Mary':1/5, 'Alice':1/5, 'Jerry':1/5}), \\\n",
    "                            ('S2',{'a':3/8, 'the':4/8, 'that':1/8}),\\\n",
    "                            ('S3',{'bit':1/6, 'saw':1/6, 'ate':1/6, 'played':1/6, 'hit':1/6, 'gave':1/6}), \\\n",
    "                            ('S4', {'cat':1/6, 'dog':1/6, 'car':1/6, 'bed':1/6, 'apple':1/6, 'pen':1/6}), \\\n",
    "                            ('Sf', {})])\n",
    "# wordProbs.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = sc.textFile('hdfs:///user/root/transition-matrix.txt')\n",
    "header = lines.first().split('\\t')\n",
    "def czip(x):\n",
    "    return tuple(zip(states, x[1:]))\n",
    "    \n",
    "trans = lines.map(lambda line: line.split('\\t')) \\\n",
    "             .filter(lambda line: line != header)\\\n",
    "             .map(czip).flatMap(lambda x:x) \\\n",
    "             \n",
    "\n",
    "numlinks = trans.filter(lambda x: x[1] !='0').map(lambda x: (x[0], 1)).reduceByKey(lambda x, y: x+y)\n",
    "trans = trans.mapValues(float).reduceByKey(lambda x, y: x+y) \n",
    "\n",
    "numlinks.collect()\n",
    "\n",
    "ranks.join(numlinks.join(trans)).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 0\n",
    "pre_ranks = ranks\n",
    "diff = 1\n",
    "def compute_contribs(pair):\n",
    "    [url, [rank, [numlinks,tran]]] = pair  # split key-value pair\n",
    "    return [(url, rank * tran / numlinks)]\n",
    "\n",
    "def rdd_diff(pair):\n",
    "    (url, (pre, cur)) = pair\n",
    "    return [abs(cur-pre)]\n",
    "while diff > 0.01:\n",
    "    contribs = ranks.join(numlinks.join(trans)).flatMap(compute_contribs)\n",
    "    ranks = contribs.reduceByKey(lambda x, y: x + y) \\\n",
    "                    .mapValues(lambda x: 0.15 + 0.85 * x)\n",
    "    diff = pre_ranks.join(ranks).flatMap(rdd_diff).mean()\n",
    "    pre_ranks = ranks\n",
    "    iters +=1\n",
    "\n",
    "ranks = ranks.sortBy(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gsutil rm -r gs://saberbf0098/sparkResults/HW2/\n",
    "ranks.saveAsTextFile('gs://saberbf0098/sparkResults/HW2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nNumber of iterations: ',iters)\n",
    "print('Ranks: ')\n",
    "ranks.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_word_prob(pair):\n",
    "    [jar, [wordProbs, rank]] = pair\n",
    "    return [(jar, [(word,prob * rank)]) for (word,prob) in wordProbs.items()]\n",
    "\n",
    "print('List of Word Probabilities in each Bucket:') \n",
    "wordProbs.join(ranks).flatMap(calc_word_prob).reduceByKey(lambda x,y: x+y)\\\n",
    "                     .sortBy(lambda x: x[0]).collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW3 Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "|Invoice|StockCode|         Description|Quantity|   InvoiceDate|Price|Customer ID|       Country|\n",
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "| 489434|    85048|15CM CHRISTMAS GL...|      12|12/1/2009 7:45| 6.95|      13085|United Kingdom|\n",
      "| 489434|   79323P|  PINK CHERRY LIGHTS|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|\n",
      "| 489434|   79323W| WHITE CHERRY LIGHTS|      12|12/1/2009 7:45| 6.75|      13085|United Kingdom|\n",
      "| 489434|    22041|\"RECORD FRAME 7\"\"...|      48|12/1/2009 7:45|  2.1|      13085|United Kingdom|\n",
      "| 489434|    21232|STRAWBERRY CERAMI...|      24|12/1/2009 7:45| 1.25|      13085|United Kingdom|\n",
      "| 489434|    22064|PINK DOUGHNUT TRI...|      24|12/1/2009 7:45| 1.65|      13085|United Kingdom|\n",
      "| 489434|    21871| SAVE THE PLANET MUG|      24|12/1/2009 7:45| 1.25|      13085|United Kingdom|\n",
      "| 489434|    21523|FANCY FONT HOME S...|      10|12/1/2009 7:45| 5.95|      13085|United Kingdom|\n",
      "| 489435|    22350|           CAT BOWL |      12|12/1/2009 7:46| 2.55|      13085|United Kingdom|\n",
      "| 489435|    22349|DOG BOWL , CHASIN...|      12|12/1/2009 7:46| 3.75|      13085|United Kingdom|\n",
      "+-------+---------+--------------------+--------+--------------+-----+-----------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\").option('header','true').load('gs://datathinks-home/online_retail_II.csv')\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records befor duplicate removal:  1067371\n",
      "Number of records after duplicate removal:  1033034\n",
      "Number of duplicate records removed:  34337\n",
      "Number of Null records:  235151\n",
      "Number of records befor missing value imputation:  1033034\n",
      "Number of records after missing value imputation:  797883\n",
      "+-------+---------+--------+---------------+-----+----------+\n",
      "|Invoice|StockCode|Quantity|    InvoiceDate|Price|CustomerID|\n",
      "+-------+---------+--------+---------------+-----+----------+\n",
      "| 489520|    22080|      10|12/1/2009 11:41| 1.65|     14911|\n",
      "| 489522|    22300|       3|12/1/2009 11:45| 2.55|     15998|\n",
      "| 489532|    16235|      60|12/1/2009 11:58| 0.21|     13394|\n",
      "| 489537|   72801G|       3|12/1/2009 12:14| 1.25|     14040|\n",
      "| 489539|    20692|      96|12/1/2009 12:18| 3.75|     15061|\n",
      "+-------+---------+--------+---------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select\\rename columns\n",
    "df = df.withColumn(\"CustomerID\", df[\"Customer ID\"]).drop(\"Customer ID\", \"Country\", \"Description\")\n",
    "n_records_before_cleaning = df.count()\n",
    "# remove duplicate records\n",
    "df = df.distinct()\n",
    "n_records_after_removing_duplicates = df.count()\n",
    "print('Number of records befor duplicate removal: ', n_records_before_cleaning)\n",
    "print('Number of records after duplicate removal: ', n_records_after_removing_duplicates)\n",
    "print('Number of duplicate records removed: ', n_records_before_cleaning-n_records_after_removing_duplicates)\n",
    "# Missing Value Imputation\n",
    "n_null_records = df.filter('CustomerID is null').count()\n",
    "print('Number of Null records: ', n_null_records)\n",
    "print('Number of records befor missing value imputation: ', df.count())\n",
    "df = df.na.drop()\n",
    "print('Number of records after missing value imputation: ', df.count())\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Calcuate Monetary Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Invoice: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- Price: float (nullable = true)\n",
      " |-- CustomerID: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Price\", df.Price.cast('float'))\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---+\n",
      "|customerID|          Monetary| id|\n",
      "+----------+------------------+---+\n",
      "|     14911|  70473.0999276936|  1|\n",
      "|     14096| 41376.32996287942|  2|\n",
      "|     15098| 40278.89999961853|  3|\n",
      "|     14063| 39920.94912147522|  4|\n",
      "|     14156|36397.009877726436|  5|\n",
      "|     17841| 34582.22974572331|  6|\n",
      "|     15760|  33628.5498046875|  7|\n",
      "|     12918|           32860.5|  8|\n",
      "|     12744| 25481.40001243353|  9|\n",
      "|     17399|    25111.08984375| 10|\n",
      "+----------+------------------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monDF = df.groupBy('customerID').agg(sum('Price').alias('Monetary'))\n",
    "monDF = monDF.orderBy(desc('Monetary'))\n",
    "tmp= monDF.withColumn(\"new_column\",lit(\"ABC\"))\n",
    "w = Window().partitionBy('new_column').orderBy(lit('A'))\n",
    "monDF = tmp.withColumn(\"id\", row_number().over(w)).drop(\"new_column\")\n",
    "\n",
    "monDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(pair):\n",
    "    (key, val) = pair\n",
    "    result = 0\n",
    "    if val < 0.15*cnt: \n",
    "        result = 1\n",
    "    if 0.15*cnt < val < 0.3*cnt: \n",
    "        result = 2\n",
    "    if 0.3*cnt < val < 0.6*cnt: \n",
    "        result = 3\n",
    "    if 0.6*cnt < val : \n",
    "        result = 4\n",
    "    return (key, result)\n",
    "\n",
    "cnt = monDF.count()\n",
    "monRDD = monDF.rdd\n",
    "monRDD = monRDD.map(lambda x:(x[0],x[2])).map(func)\n",
    "# monRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|customerID|Monetary|\n",
      "+----------+--------+\n",
      "|     14911|       1|\n",
      "|     14096|       1|\n",
      "|     15098|       1|\n",
      "|     14063|       1|\n",
      "|     14156|       1|\n",
      "|     17841|       1|\n",
      "|     15760|       1|\n",
      "|     12918|       1|\n",
      "|     12744|       1|\n",
      "|     17399|       1|\n",
      "+----------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD code to compute aggregate average\n",
    "monDF = monRDD.toDF([\"customerID\", \"Monetary\"])\n",
    "monDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Calcuate Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---+\n",
      "|customerID|Frequency| id|\n",
      "+----------+---------+---+\n",
      "|     17841|    12638|  1|\n",
      "|     14911|    11444|  2|\n",
      "|     12748|     6662|  3|\n",
      "|     14606|     6500|  4|\n",
      "|     14096|     5128|  5|\n",
      "|     15311|     4579|  6|\n",
      "|     14156|     4118|  7|\n",
      "|     14646|     3890|  8|\n",
      "|     13089|     3391|  9|\n",
      "|     16549|     3098| 10|\n",
      "+----------+---------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "frqDF = df.groupBy('customerID').agg(count('invoice').alias('Frequency'))\n",
    "frqDF = frqDF.orderBy(desc('Frequency'))\n",
    "tmp= frqDF.withColumn(\"new_column\",lit(\"ABC\"))\n",
    "w = Window().partitionBy('new_column').orderBy(lit('A'))\n",
    "frqDF = tmp.withColumn(\"id\", row_number().over(w)).drop(\"new_column\")\n",
    "\n",
    "frqDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = frqDF.count()\n",
    "frqRDD = frqDF.rdd\n",
    "frqRDD = frqRDD.map(lambda x:(x[0],x[2])).map(func)\n",
    "# frqRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|customerID|Frequency|\n",
      "+----------+---------+\n",
      "|     17841|        1|\n",
      "|     14911|        1|\n",
      "|     12748|        1|\n",
      "|     14606|        1|\n",
      "|     14096|        1|\n",
      "|     15311|        1|\n",
      "|     14156|        1|\n",
      "|     14646|        1|\n",
      "|     13089|        1|\n",
      "|     16549|        1|\n",
      "+----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD code to compute aggregate average\n",
    "frqDF = frqRDD.toDF([\"customerID\", \"Frequency\"])\n",
    "frqDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Calcuate Recency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|CustomerID|   Recency|\n",
      "+----------+----------+\n",
      "|     12713|2011-12-09|\n",
      "|     12985|2011-12-09|\n",
      "|     13298|2011-12-08|\n",
      "|     14251|2011-12-08|\n",
      "|     14138|2011-12-08|\n",
      "|     15877|2011-12-08|\n",
      "|     15520|2011-12-08|\n",
      "|     16322|2011-12-08|\n",
      "|     15156|2011-12-08|\n",
      "|     13521|2011-12-08|\n",
      "+----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setting an user define function:\n",
    "# This function converts the string cell into a date:\n",
    "dateFormat =  udf (lambda x: dt.strptime(x, '%m/%d/%Y %H:%M'), DateType())\n",
    "\n",
    "recDF = df.groupBy('CustomerID').agg(max('InvoiceDate').alias('Recency'))\n",
    "recDF = recDF.select('CustomerID','Recency') \\\n",
    "        .withColumn('Recency', dateFormat(col('Recency')))\n",
    "recDF = recDF.orderBy(desc('Recency'))\n",
    "recDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compDate(pair):\n",
    "    (key, val) = pair\n",
    "    result = 0\n",
    "    if dt.date(dt(2011, 11, 15)) <= val: \n",
    "        result = 1\n",
    "    if dt.date(dt(2011, 9, 5)) <= val <= dt.date(dt(2011, 11, 14)): \n",
    "        result = 2\n",
    "    if dt.date(dt(2011, 1, 5)) <= val <= dt.date(dt(2011, 9, 4)): \n",
    "        result = 3\n",
    "    if val <= dt.date(dt(2011, 1, 4)): \n",
    "        result = 4\n",
    "    return (key, result)\n",
    "\n",
    "\n",
    "# func =  udf (compDate, DateType())\n",
    "# recDF.withColumn('Recency', func(col('Recency'))).show(10)\n",
    "\n",
    "recRDD = recDF.rdd\n",
    "recRDD = recRDD.map(lambda x:(x[0],x[1])).map(compDate)\n",
    "# recRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|customerID|Recency|\n",
      "+----------+-------+\n",
      "|     12713|      1|\n",
      "|     12985|      1|\n",
      "|     16322|      1|\n",
      "|     15156|      1|\n",
      "|     13471|      1|\n",
      "|     13521|      1|\n",
      "|     15877|      1|\n",
      "|     15520|      1|\n",
      "|     14251|      1|\n",
      "|     14138|      1|\n",
      "+----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# RDD code to compute aggregate average\n",
    "recDF = recRDD.toDF([\"customerID\", \"Recency\"])\n",
    "recDF.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Number of customers in each category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+---------+--------+\n",
      "|customerID|Recency|Frequency|Monetary|\n",
      "+----------+-------+---------+--------+\n",
      "|     12394|      3|        4|       4|\n",
      "|     12529|      4|        4|       4|\n",
      "|     12847|      1|        2|       3|\n",
      "|     13192|      2|        2|       2|\n",
      "|     13282|      2|        3|       3|\n",
      "|     13442|      4|        3|       3|\n",
      "|     13610|      4|        1|       1|\n",
      "|     13772|      3|        1|       1|\n",
      "|     13865|      3|        4|       4|\n",
      "|     14157|      4|        3|       3|\n",
      "+----------+-------+---------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "uDF = recDF.join(frqDF, on=['CustomerID'], how='outer')\n",
    "uDF = uDF.join(monDF, on=['CustomerID'], how='outer')\n",
    "uDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "BestCustomer = uDF.where((col('Recency')==lit('1')) \\\n",
    "            & (col('Frequency')==lit('1')) \\\n",
    "            & (col('Monetary')==lit('1'))).count()\n",
    "\n",
    "LoyalCustomer = uDF.where((col('Frequency')==lit('1'))).count()\n",
    "\n",
    "BigSpender = uDF.where((col('Monetary')==lit('1'))).count()\n",
    "\n",
    "AlmostLost = uDF.where((col('Recency')==lit('3')) \\\n",
    "            & (col('Frequency')==lit('1')) \\\n",
    "            & (col('Monetary')==lit('1'))).count()\n",
    "\n",
    "LostCustomers = uDF.where((col('Recency')==lit('4')) \\\n",
    "            & (col('Frequency')==lit('1')) \\\n",
    "            & (col('Monetary')==lit('1'))).count()\n",
    "\n",
    "LostCheapCustomers = uDF.where((col('Recency')==lit('4')) \\\n",
    "            & (col('Frequency')==lit('4')) \\\n",
    "            & (col('Monetary')==lit('4'))).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Best Customers:  6\n",
      "Number of Loyal Customers:  891\n",
      "Number of Big Spenders:  891\n",
      "Number of Almost Lost:  103\n",
      "Number of Lost Customers:  371\n",
      "Number of Lost Cheap Customers:  1150\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of Best Customers: \", BestCustomer)\n",
    "print(\"Number of Loyal Customers: \", LoyalCustomer)\n",
    "print(\"Number of Big Spenders: \", BigSpender)\n",
    "print(\"Number of Almost Lost: \", AlmostLost)\n",
    "print(\"Number of Lost Customers: \", LostCustomers)\n",
    "print(\"Number of Lost Cheap Customers: \", LostCheapCustomers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## PA4 Question 4\n",
    "\n",
    "A freshly-loaded copy of the NYT covid dataset is available as [gs://datathinks-home/covid2.json](gs://datathinks-home/covid2.json).\n",
    "\n",
    "Please don’t upload your data! Instead, starting on March 1, 2020, for the first day of each month, which county had the worst numbers of confirmed cases, and deaths? They might not be the same county. In other words, develop a table that looks like this:\n",
    "\n",
    "| Query | 4/1 | 5/1 | ...etc... |\n",
    "| --- | --- | --- | --- |\n",
    "| Confirmed Cases | nnn, County, State | nnn, County, State |   |\n",
    "| Deaths | nnn, County, State | nnn, County, State |   |\n",
    "\n",
    "This analysis should be done on Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 Question 1\n",
    "\n",
    "The last 5 presidents in our speeches collection were `reagan, bush, clinton, gwbush` and `obama` (that's how the tar.gz files of their speeches are named in Canvas).\n",
    "\n",
    "Pairwise comparisons of the similarities in their speech collections (10 pairs) will give us a half-matrix like shown below (The symmetry in the problem formulation makes it unnecessary to compute the blank spots in the matrix):\n",
    "\n",
    "|  | r | b | c | g | o |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| reagan |   | * | * | * | * |\n",
    "| bush |   |   | * | * | * |\n",
    "| clinton |   |   |   | * | * |\n",
    "| gwbush |   |   |   |   | * |\n",
    "\n",
    "Compute the similarity denoted by each asterisk and answer:\n",
    "\n",
    "1. Using n-gram character shingles, assuming n=4, which two presidents' speeches were the most similar and which were the least similar?\n",
    "2. Using n-gram word shingles, assuming n=3, which two presidents' speeches were the most similar and which were the least similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 Question 3\n",
    "\n",
    "This question builds on the [UCI Online Retail II dataset](https://archive.ics.uci.edu/ml/datasets/Online+Retail+II) analysis you performed in Quiz 3. This time, however, the R, F, M values should be calculated as follows:\n",
    "\n",
    "Recency should be the number of days relative to year-end 2011 (Dec 31). \n",
    "Frequency should simply be the number of transactions in the total period.\n",
    "Monetary value should be the log10 of the total dollars spent. Why log10? We use logs to flatten the range — so high-spenders don't skew the analysis.\n",
    "After calculating RFM values as specified above, run K-means clustering to divide the customers into 6 clusters. How do the number of customers in these 6 clusters compare with the clusters you in Question 2 of Quiz 3?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA4 Question 4\n",
    "\n",
    "A freshly-loaded copy of the NYT covid dataset is available as [`gs://datathinks-home/covid2.json`](gs://datathinks-home/covid2.json).\n",
    "\n",
    "Please don’t upload your data! Instead, starting on March 1, 2020, for the first day of each month, which county had the worst numbers of confirmed cases, and deaths? They might not be the same county. In other words, develop a table that looks like this:\n",
    "\n",
    "| Query | 4/1 | 5/1 | ...etc... |\n",
    "| --- | --- | --- | --- |\n",
    "| Confirmed Cases | nnn, County, State | nnn, County, State |   |\n",
    "| Deaths | nnn, County, State | nnn, County, State |   |\n",
    "\n",
    "This analysis should be done on Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3c5d29de0d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# to read the .json file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcovidDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gs://datathinks-home/covid2.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# covidDF = spark.read.load('gs://datathinks-home/covid2.json',\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#                           format='json', inferSchema='true', header='true')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1255\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1257\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    986\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/anaconda/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# to read the .json file\n",
    "covidDF = spark.read.json('gs://datathinks-home/covid2.json')\n",
    "# covidDF = spark.read.load('gs://datathinks-home/covid2.json', \n",
    "#                           format='json', inferSchema='true', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-0098-m.us-east1-b.c.bigdata-09012020.internal:45647\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f5454988710>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
