{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import sys\n",
    "import datetime\n",
    "import time \n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.streaming import StreamingContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as sFuncs\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cluster-0098-m.us-east1-b.c.bigdata-09012020.internal:38931\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=yarn appName=PySparkShell>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA5 Question 1\n",
    "\n",
    "Moving Averages\n",
    "\n",
    "Calculation of moving stock price averages are part of many a trading strategies ([reference](https://www.investopedia.com/articles/active-trading/052014/how-use-moving-average-buy-stocks.asp)).\n",
    "\n",
    "We will be using the two moving averages strategy, with the shorter-term MA being 10-day and the longer average being 40-day. When the shorter-term MA crosses above the longer-term MA, it's a buy signal, as it indicates that the trend is shifting up. This is known as a \"golden cross.\"\n",
    "\n",
    "Meanwhile, when the shorter-term MA crosses below the longer-term MA, it's a sell signal, as it indicates that the trend is shifting down. This is known as a \"dead/death cross.\"\n",
    "\n",
    "To simulate a data stream, you are given a python program `stream-feeder.py` which reads in `dj30.csv` file and pipes it, line by line. `dj30.csv` contains a 25-year history of the Dow Jones Industrial Average prices. We will only be concerned with the Close price. The command `stream-feeder.py | nc -lk 9999` can be run on the master machine of your spark cluster to feed the Close data into pyspark.\n",
    "\n",
    "1. Set up the stream to feed data into a pyspark DStream. Write and submit a summary of the steps you took (in English) and enclose the (cleaned up after editing) output of `history > /tmp/my_session.txt`. This history should include what you typed into the shell outside of the pyspark session. \\[2 pts\\]\n",
    "2. Use DStream windowing to separately accumulate the sum and count of prices, thus creating moving average DStreams. Write and submit the (cleaned up after editing) transcript of your session along with your code. \\[4 pts\\]\n",
    "3. \\[Optional, 4 bonus points\\]. Compare the two moving averages to indicate buy and sell signals. Your output should be of the form `[( <date> buy), ( <date> sell), etc]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data\n",
    "Note that this below cell needs to be run once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to unpack the dataset into the current directory\n",
    "# NOTE that this cell needs to run once\n",
    "# %%bash\n",
    "# cd /home/saberbf/BigData/PA5\n",
    "# sudo apt-get install python3-pip\n",
    "# pip3 install pandas\n",
    "# pip3 install feedparser\n",
    "# gsutil cp gs://datathinks-home/stream-feeder.py .\n",
    "# gsutil cp gs://datathinks-home/dj30.csv .\n",
    "# gsutil cp gs://datathinks-home/headline-extractor.py .\n",
    "# gsutil cp gs://datathinks-home/feed-parser.py .\n",
    "# gsutil cp gs://datathinks-home/2020-headlines.csv ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Spark Streaming Context\n",
    "\n",
    "##### Notes:\n",
    "- In case you are using a single-node with k many threads cluster, it is essential to use setMaster('local[k]') or less than k. Otherwise, SparkContext put the sc.master on 'yarn'. It doesn't consider threads as workers and looks for individual workers to do the job. The result would be you will not see a collect() to converge.\n",
    "- master is a Spark, Mesos or YARN cluster URL, or a special “local[*]” string to run in local mode. In practice, when running on a cluster, you will not want to hardcode master in the program, but rather launch the application with spark-submit and receive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming in-process (detects the number of cores in the local system)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 7), ('a', 2), ('b', 2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc._conf.getAll()\n",
    "sc.stop()\n",
    "# Create a local StreamingContext with 4 working threads\n",
    "conf = SparkConf().setMaster('local[*]')\n",
    "sc = SparkContext(conf=conf, appName='NetworkWordCount')\n",
    "\n",
    "# test the SparkContext to see if it works\n",
    "rdd = sc.parallelize([('a',7),('a',2),('b',2)])\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to load the data into a DataFrame\n",
    "spark = SparkSession.builder.master('local[4]').getOrCreate()\n",
    "dj30DF = spark.read.load('gs://datathinks-home/dj30.csv', \n",
    "                     format='csv', inferSchema=True, header=True, delimiter=',')\n",
    "dj30DF = dj30DF.select(['Close', \n",
    "           'Date', \n",
    "           'Long Date', \n",
    "           'Total Stks above 30MA', \n",
    "           'Total Stks below 30MA', \n",
    "           'Total Stks equal 30MA'])\n",
    "\n",
    "# dj30DF.withColumn('movingAverage', sum(dj30DF[Close])).over(Window.rowsBetween(-10,0)).show(10)\n",
    "dj30DF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local StreamingContext with batch interval of 10 seconds\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "# Split each line into words and transform to (word,count) tuple\n",
    "words = lines.flatMap(lambda line: line.split(\" \"))\n",
    "# reduce last 30 seconds of data, every 10 seconds\n",
    "# Counting the number of elements in each RDD of the source DStream\n",
    "words.count().pprint()\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination(timeout=150)  # Wait for the computation to terminate\n",
    "\n",
    "\n",
    "## it returns 380 and 382 for two RDDs in this cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()#(stopSparkContext=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local StreamingContext with batch interval of 2.5 seconds which should hold to 10 sample per RDD\n",
    "ssc = StreamingContext(sc, 2.5)\n",
    "\n",
    "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
    "lines = ssc.socketTextStream(\"localhost\", 9999)\n",
    "# Split each line into words\n",
    "words = lines.flatMap(lambda line: line.split(\" \")).map(lambda word:(word, 1))\n",
    "# reduce last 30 seconds of data, every 10 seconds\n",
    "priceSum = words.reduceByWindow(lambda x, y: (float(x[0])+float(y[0]), int(x[1])+int(y[1])),\n",
    "                              None,#lambda x, y: (float(x[0])-float(y[0]), int(x[1])-int(y[1])),\n",
    "                              windowDuration=10,\n",
    "                              slideDuration=2.5)\n",
    "# Print the first ten elements of each RDD generated in this DStream to the console\n",
    "priceSum.pprint()\n",
    "ssc.start()             # Start the computation\n",
    "ssc.awaitTermination()  # Wait for the computation to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PA5 Question 2\n",
    "\n",
    "Notable News\n",
    "\n",
    "Most news outlets distribute their news through rss feeds for use by news reader programs. We are writing a news reader that reads the news headlines and only reports those headlines that contain an unfamiliar word. (It's not going to be all that useful but hey...)\n",
    "\n",
    "The file `2020-headlines.csv` contains headlines from 2020 for mining for familiar words and `headline-extractor.py` for extracting words from such headlines. The program is only half-written. Add to it as follows:\n",
    "\n",
    "1. Create a Bloom Filter string whose size is approximately 8 times the number of understood words and write the buffer into a text file `bloom.txt` in your shell.\n",
    "2. The file `bloom.txt` will be used in pyspark. You may want to store it in hdfs so it is accessible from pyspark.\n",
    "To simulate a data stream, you are given a python program `feed-parser.py` which reads rss feeds from several news outlets. It is rate controlled, feeding us 4 titles per second. The command `feed-parser.py | nc -lk 9999` can be run on the master machine of your spark cluster to feed the titles data into pyspark.\n",
    "\n",
    "1. Set up the stream to feed data into a pyspark DStream. Write and submit a summary of the steps you took (in English) and enclose the (cleaned up after editing) output of `history > /tmp/my_session.txt`. This history should include what you typed into the shell outside of the pyspark session. \\[no points for this\\]\n",
    "2. Use DStream windowing to filter incoming headlines. Use a Bloom filter based on `bloom.txt` to emit only the headlines with unfamiliar words in them. Write and submit the (cleaned up after editing) transcript of your session along with your code. \\[4 pts\\]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
